<!DOCTYPE html>
<html lang='en_US'>
  <head>
    <title>Linux - Stelfox Athen&#xe6;um</title>

    <meta http-equiv='content-type' content='text/html; charset=utf-8' />
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1' />
    
    

    <link rel="alternate" type="application/atom+xml" title="Stelfox Athen&#xe6;um Feed" href="/atom.xml" />
    <link rel="canonical" href="https://stelfox.net" />
    <link rel="author" href="https://plus.google.com/+SamStelfox31337/"/>

    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Linux" />
    <meta property="og:url" content="https://stelfox.net/tags/linux/" />
    <meta property="og:site_name" content="Stelfox Athen&#xe6;um" />
    <meta property="article:publisher" content="https://www.facebook.com/sstelfox" />
    <meta property="og:image" content="https://stelfox.net/static/avatar-01.jpg" />

    <link rel='stylesheet' href="//fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Alegreya+SC:700" />
    <link rel="stylesheet" href="https://stelfox.net/css/3HrCRVwL3Gos2V4HzXAw/Om/nxs=.css" />

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
       (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-32188490-1', 'stelfox.net');
      ga('require', 'displayfeatures');
      ga('send', 'pageview');
    </script>
  </head>
  <body>
    <header class='masthead'>
      <div class='masthead-inner'>
        <h1><a href='/' title='Home'>Sam Stelfox</a></h1>
        <p class='lead'>Thoughts from a systems hacker and developer.</p>
        <nav>
          <ul>
            <li><a href="/about/" title="About">About</a></li>
            <li><a href="/blog/" title="Blog">Blog</a></li>
            <li><a href="/knowledge_base/" title="Knowledge Base">Knowledge Base</a></li>
            <li><form action="/search/" method="get"><input type="search" name="q" placeholder="Search" /></form></li>
          </ul>
        </nav>
        <footer class='colophon'>
          <p>&copy; 2014, All rights reserved.</p>
        </footer>
      </div>
    </header>
    <div class='content container'>
      <div class="post">
  <h1>Linux</h1>
  
    <div class='post'>
      <h2><a href="/blog/2014/02/setting-linux-system-timezone/">Setting Linux System Timezone</a></h1>
      <aside>Posted at: 2014-02-01 13:50:46 -0500</aside>
      <article>
        <p>I change the timezone on the linux systems so rarely that I almost always have
to look it up. I&#39;m writing it up here for my own personal reference. With any
luck it&#39;ll also help others.</p>

<p>The system timezone is controlled by the <code>/etc/localtime</code> file and is generally
symlinked to locale files stored in <code>/usr/share/zoneinfo</code>. Generally I like to
keep my systems on UTC as I my machines are in several timezones and it makes
all the logs have consistent times.</p>

<p>To set the system time to UTC you&#39;d run the following command as root:</p>
<div class="CodeRay">
  <div class="code"><pre>ln -sf /usr/share/zoneinfo/UTC /etc/localtime
</pre></div>
</div>

<p>Other timezones can be found in the <code>/usr/share/zoneinfo</code> and are generally
broken up by continent with a few exceptions.</p>

<p>As a user it&#39;s obviously more useful to see the time in my local timezone and
this can be overridden on a per-user basis using the TZ environment variable. I
stick this in my <code>~/.bashrc</code> file and it just works transparently:</p>
<div class="CodeRay">
  <div class="code"><pre>export TZ=&quot;America/Los_Angeles&quot;
</pre></div>
</div>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2014/06/modifying-the-hosts-file-in-a-docker-container/">Modifying the Hosts File in a Docker Container</a></h1>
      <aside>Posted at: 2014-06-03 11:43:59 -0400</aside>
      <article>
        <p>Before I describe the issue that I encountered, let me be very clear. This hack
is <em>potentially dangerous and should absolutely only be done in development
environments</em>. This won&#39;t affect your host system, only the docker container so
the most damage you&#39;ll do is prevent hostname and possibly user/group lookups
within the container itself.</p>

<p>Alright with that out of the way, I was actively working on a codebase that
uses subdomains as part of the identifier. Rather than setup a full DNS server,
point my local system at it and load in the domains I wanted to simply modify
the /etc/hosts file inside the environment.</p>

<p>Docker mounts an /etc/hosts file inside it&#39;s containers, read-only, and the
container&#39;s &#39;root&#39; user has had it&#39;s mount permissions revoked so it&#39;s not able
to be modified. Other users have encountered this issue, and a <a href="https://stackoverflow.com/questions/19414543/how-can-i-make-etc-hosts-writable-by-root-in-a-docker-container">novel
workaround was put forward</a>. The solution however makes use of perl, and is
specific too ubuntu base systems.</p>

<p>I&#39;ll explain the solution after showing a more general way to accomplish the
same thing. Different linux systems will store their libraries in different
directory structures. CentOS is different from Fedora, which is different from
Ubuntu and Debian. All of them name their libraries, in this case we&#39;re looking
for &#39;libnss_files.so.2&#39;.</p>

<p>You can find where your copy of this library lives with the following command.
This should be run inside the docker container that you want to modify the
/etc/hosts file in.</p>
<div class="CodeRay">
  <div class="code"><pre>find / -name libnss_files.so.2 -print 2&gt; /dev/null
</pre></div>
</div>

<p>Pay attention to the path, multiple files may show up and you want the one that
matches your system&#39;s running kernel (generally x86_64 systems will have their
libraries in a lib64 directory).</p>

<p>Once you&#39;ve found this add the following lines to your Dockerfile. Make sure
you modify the path in the copy in the first line to the path of your copy of
the library. Once done you&#39;ll use the /var/hosts file to modify your hosts file
instead.</p>
<div class="CodeRay">
  <div class="code"><pre>RUN mkdir -p /override_lib &amp;&amp; cp /etc/hosts /var/ &amp;&amp; cp /usr/lib64/libnss_files.so.2 /override_lib
RUN sed -ie 's:/etc/hosts:/var/hosts:g' /override_lib/libnss_files.so.2
ENV LD_LIBRARY_PATH /override_lib
</pre></div>
</div>

<p>So what is this actually doing? On linux systems, name configurations such as
DNS, username, and group lookups are generally handled by the &#39;nss&#39; or name
service switch configuration tools including the hosts file. The library that
we&#39;re copying and modifying is a very specific to reading from files on the
system and includes the default paths to these files.</p>

<p>Generally you have to be very careful when you&#39;re manipulating strings within
compiled libraries. The length of the string is encoded along with it, so at a
minimum it&#39;s important that the string is <em>the same length or less</em>. You can
get away with less but it requires additionally writing an end of string
character as well.</p>

<p>Too make this hack simple, we&#39;re simply replacing the &#39;etc&#39; with &#39;var&#39;, both
systems directories that regular users generally should have read access but
not write access too.</p>

<p>Finally we need to tell all programs that need to perform lookups using
hostnames in the hosts file to make use of our modified library instead of the
system one. Linux will look for shared libraries at runtime in any paths set in
in the LD_LIBRARY_PATH (colon delimited just like PATH) and this doesn&#39;t
require any privileges too set.</p>

<p>And the result? An editable hosts file, with no extra services. I can&#39;t stress
enough though, there could be bad ramifications from modifying libraries this
way. This is definitely not a &#39;production ready&#39; hack.</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2013/12/updating-bmc-on-dell-poweredge-c6100/">Updating BMC on Dell PowerEdge C6100</a></h1>
      <aside>Posted at: 2013-12-16 21:26:13 -0500</aside>
      <article>
        <p>I just received my Dell PowerEdge C6100 and found it&#39;s software quite a bit
outdated. After searching around quite a bit I found the resources lacking for
explaining how to perform these updates. So in this post I&#39;m going to quickly
cover updating the BMC firmware on each blade.</p>

<p>The system I received had four different versions of the BMC software
installed, additionally Two were branded as MegaRAC and the others branded as
Dell. This update didn&#39;t fix the branding (and I&#39;d love to remove the Dell
branding as it&#39;s kind of annoying) it did, however, fix a number of other
issues that I was experiencing such as:</p>

<ol>
<li>Console Redirection failing to connect</li>
<li>BMC losing it&#39;s network connection after a couple of minutes</li>
<li>Slow responses, with occasional failures to load pages</li>
<li>Remote IPMI tools being unable to read sensors status</li>
</ol>

<p>The first step is too download the latest version of of the BMC software from
<a href="https://support.dell.com/">Dell&#39;s support site</a> (Or a <a href="http://downloads.dell.com/Pages/Drivers/poweredge-c6100-all.html">direct link</a>, I&#39;ve also taken the liberty of
<a href="http://static.stelfox.net/files/PEC6100BMC130.exe">hosting a copy myself</a>). I recommend you go through the process of entering
the service tag of each of the blades and make sure that Dell recognizes them
as existing even if they&#39;re out of support.</p>

<p>There has been mention of versions of these blades that had custom
modifications for DCS and any attempts to modify the BIOS or BMC will likely
cause you to end up bricking the remote management board or the motherboard.</p>

<p>Even with the regular board there is always a risk of bricking it, though
firmware updates have gotten a lot more reliable and I haven&#39;t experienced a
mis-flashed motherboard in years. You&#39;ve been warned.</p>

<p>The BMC was fairly straight-foward. I installed the 64-bit version of Fedora 19
on a thumbdrive, downloaded version 1.30 of the BMC software (get the file
named <code>PEC6100BMC130.exe</code>). The file itself is a self-extracting zip archive
which can be extracted using the regular unzip utility.</p>
<div class="CodeRay">
  <div class="code"><pre>unzip PEC6100BMC130.exe
</pre></div>
</div>

<p>Inside you&#39;ll find two folders, KCSFlash and SOCFlash should both be put on the
live drive within the KCSFlash. You&#39;ll need to set the execute bit on the
contents of the linux directory and the linux.sh file. You&#39;ll also need to
install the <code>glibc.i686</code> package. Afterwards it&#39;s as simple as booting each
chassis off the drive and as root run the linux.sh script.</p>

<p>If the KCSFlash fails, the SOCFlash will more likely than not work but it is
slightly more dangerous. If you need it mark the <code>linux/flash8.sh</code>,
<code>linux/socflash</code>, and <code>linux/socflash_x64</code> as executable in the SOCFlash folder
and run the flash8.sh script.</p>

<p>After that you&#39;re going to want to reboot into the BIOS and ensure the IPMI
ethernet port is set to dedicated, as this switched it back to &quot;Shared&quot; on me.</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2009/11/image-crawler-meets-rm-f-star/">Image Crawler Meets rm -f *</a></h1>
      <aside>Posted at: 2009-11-04 16:36:45 -0500</aside>
      <article>
        <p>So for giggles I wrote a simple web crawler that archived any image it found on
a site that rapidly updated. My plan was to take all of the images resize them
to a fraction of what they were and make a collage progression of the images
posted over a 24 hours period. If that was successful I was going to try and
sort the image by the highest peak on a gray scale histogram of the image.</p>

<p>I let my crawler go and stopped it after a 24 hour period. I ls&#39;d the directory
the images were being saved in to see how many I got and my ssh session locked
up... Or so I thought. I hit Ctrl - C and nothing happened... So I closed my
window and opened a new one. Did the same ls and the same thing happened. At
this point I was really confused, did my system get compromised?  Maybe a
trojan ls was put on my system that was broken somehow?</p>

<p>The last thought put me into a panic, I raced to a system that held incremental
backups of this entire system and ran an md5sum on /bin/ls on it and on every
ls that I could find in the backup. There hadn&#39;t been any change. At this point
I was fairly certain that wasn&#39;t the case so I moved on...</p>

<p>Maybe there was a filename that hit some weird glitch in ls&#39;s programming
causing it too lock up. If this was the case how should I go about fixing it? I
started thinking about how it would be a nice contribution to the community if
I could figure out the error and report it, but I decided to take the lazy way
out.</p>

<p>I cd&#39;d to the directory and ran an rm -f * and was greeted with this:</p>

<blockquote>
<p>/bin/rm: Argument list too long.</p>
</blockquote>

<p>Wait what? All I get was this irritating and slightly elusive error message. So
the * was being expanded, and making the list too long? To be sure I started
hunting in the man pages. <code>man rm</code> recommended me to <code>info coreutils &#39;rm
invocation&#39;</code>. I read through and couldn&#39;t find any limitations or warnings that
might relate to the problem. The only thing I could find in there was a line
that said:</p>

<blockquote>
<p>GNU <code>rm&#39;, like every program that uses the</code>getopt&#39; function to parse its
arguments...</p>
</blockquote>

<p>Alrighty moving on... so getopt is parsing it&#39;s options... man and info pages
on getopt don&#39;t really reveal anything... So to Google!</p>

<p>After a bit of googling I found the answer, getopt&#39;s argument limit is 1024.
So... how many files did I have? I wanted to give ls one more try... I typed it
in and sure enough console froze... or did it? I walked away and did other
things. When I came back I had a large list of files, longer than my console
buffer... Ok-day, lets try this again &#39;ls -l | wc -l&#39;. After about five minutes
it came back again with just the number.</p>

<blockquote>
<p>177654</p>
</blockquote>

<p>Whoa... I wasn&#39;t expecting that... So how do I get around it... &#39;find . | xargs
rm -f&#39; and.... WOO Victory! The directory is clean and happy again... Now i&#39;ll
just have to figure out how to cut down the number of files, or at the very
least organize them into more managable chunks...</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2014/03/preventing-tmux-lockups/">Preventing Tmux Lockups</a></h1>
      <aside>Posted at: 2014-03-28 12:56:12 -0400</aside>
      <article>
        <p>Anyone that has used SSH, Tmux or Screen for a while will have inevitably
dumped excessive output to their terminal. Depending on the size of the output
you may have experienced the dreaded lockup. That horrible realization seconds
after you hit the command where signals just stop working and you just have to
sit there and wait for your terminal to catch up.a</p>

<p>There is a piece of remote connection software called Mosh that I&#39;ve been told
handles this pretty well, but I don&#39;t yet trust its security model and it
doesn&#39;t prevent the same thing from happening locally.</p>

<p>This is especially bad if you&#39;re working in a multi-pane tmux window as it&#39;s
locks up all the terminals in the same window, and prevents you from changing
to the other windows.</p>

<p>I&#39;ve had this issue happen to me one too many times but never thought of
looking for a solution until a friend of mine, <a href="http://gabekoss.com/">Gabe Koss</a>, made a passing
comment along the lines of &quot;Too bad tmux can&#39;t rate limit the output of a
terminal&quot;.</p>

<p>A quick search through the doc and two relatively recent configuration options
popped out doing exactly what I was looking for (c0-change-internal, and
c0-change-trigger). Googling around for good values, left me wanting. A lot of
people were recommending setting the values to 100 and 250 respectively; These
are the defaults and since I still experience the issue are clearly not working
for me.</p>

<p>To set the variables to something more reasonable I had to understand what they
were doing. A &#39;C0&#39; sequence is one that modifies the screen beyond a normal
character sequence, think newlines, carriage returns, backspaces. According to
the tmux man page, the trigger will catch if the number of c0 sequences per
<strong>millisecond</strong> exceeds the number in the configuration file, at which point it
will start displaying an update once every interval number of milliseconds.</p>

<p>I can&#39;t see faster than my eye&#39;s refresh rate so that seems like a decent
starting point. According to <a href="http://en.wikipedia.org/wiki/Frame_rate">wikipedia</a> the human eye/brain interface can
process 10-12 images per second but we can notice &#39;choppiness&#39; below 48 FPS.
Since I won&#39;t be reading anything flying by that fast I settled on a maximum
rate of 10 FPS updated in my shell, or an interval of &#39;100ms&#39;.</p>

<p>For the trigger I was signficantly less scientific, I dropped the trigger by
50, reloaded my tmux configuration, cat&#39;d a large file and tested whether I
could immediately kill the process and move between panes. I finally settled on
a value of &#39;75&#39; for the trigger rate. It does make the output seem a little
choppy but it is signficantly nicer to not kill my terminal.</p>

<p>TL;DR Add the following lines to your ~/.tmux.conf file and you&#39;ll be in a much
better shape:</p>
<div class="CodeRay">
  <div class="code"><pre>setw -g c0-change-interval 50
setw -g c0-change-trigger  75
</pre></div>
</div>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2014/05/chain-loading-kernels/">Chain Loading Kernels</a></h1>
      <aside>Posted at: 2014-05-23 11:39:16 -0400</aside>
      <article>
        <p>I&#39;ve found several places where I needed to be able to update my kernels but
for one reason or another can&#39;t update the kernel that gets booted initially. A
couple of these situations were:</p>

<ul>
<li>Running Custom or Updated Kernels on DigitalOcean (this is one of their
biggest failings IMHO)</li>
<li>Allowing updating of kernels on embedded linux devices that require their
kernel flashed into NVRAM.</li>
<li>Running an embedded system that used an active/backup partition scheme for
updating.</li>
</ul>

<p>In all cases the process was pretty much the same, though there were some
custom changes to the preliminary init system depending on what I needed to get
done, especially with the last one which I may cover in a different article.</p>

<p>In all cases these were done on a RedHat based distribution like CentOS,
Scientific Linux, RHEL, or even Fedora. For those users of Debian based systems
you&#39;ll need to adjust the scripts too your system though I can&#39;t imagine
anything other than the package names changing.</p>

<p>This assumes you already have the kernel and initramfs you want to boot
installed on your local filesystem at <code>/boot/vmlinuz-custom</code> and
<code>/boot/initramfs.img</code>.</p>

<p>A quick background on how this works, when the linux kernel is compiled an init
program is configured to be the first thing triggered, by default and in most
situations this will be the executable <code>/sbin/init</code>. This init process is then
responsible for starting the rest of the daemons and processes that make up the
systems we regularly interact with.</p>

<p>There are tools that allow you too effectively execute another kernel to run in
place of the kernel that is already running. There are some catches though as
the new kernel won&#39;t always re-initialize all devices (since they&#39;ve already
been initialized) and that can lead too some weird behaviors with processes
that already have hooks on those devices.</p>

<p>Too prevent any issues you need to load the new kernel as early in the boot
process as possible. Doing this in the init program is pretty much as early as
you can get and makes for a pretty stable system (I&#39;ve yet to experience any
issues with machines running this way).</p>

<p>There are several different init systems and they all behave a little
differently, as far as I know only systemd supports a means of automatically
executing a different kernel but I am personally not a systemd fan and it would
be too late in the boot process already for me too trust the chain load. You
can reliably chain load kernels regardless of what your normal init system is
though very easily and that&#39;s what I&#39;m going to cover here.</p>

<p>You&#39;ll need to have the kexec tools installed on your system. This is pretty
straight-forward:</p>
<div class="CodeRay">
  <div class="code"><pre>yum install kexec-tools -y
</pre></div>
</div>

<p>Next we&#39;re going to shift the standard init process off to the side, someplace
still accessible so we can call it later (this will need to be done as root).</p>
<div class="CodeRay">
  <div class="code"><pre>mv /sbin/init /sbin/init.original
</pre></div>
</div>

<p>Now we need to create our own init script that will handle detecting if it&#39;s
the new or old kernel, replacing the kernel if it is indeed an old one, and
starting up the normal init process if it&#39;s the new kernel.</p>

<p>Now there is a very important catch here, whatever process starts up first is
given PID 1 which is very important in kernel land. Whatever process is PID 1
will inherit all zombie processes on the system and will need to handle them.
Since our shell script is the first thing started up it will get PID 1 for both
the old and new kernel and getting the process handling code correct is not a
trivial issue.</p>

<p>What we really need is to hand over PID 1 to the init process so it can do it&#39;s
job normally as if the shell script never existed. There is a native function
to do exactly this in these shell scripts: <code>exec</code>.</p>

<p>Our simple shell script to do the chain load looks like this:</p>
<div class="CodeRay">
  <div class="code"><pre>#!/bin/bash

# Detect if this is the old kernel (not booted with the otherwise meaningless
# 'kexeced' parameter.
if [ $(grep -q ' kexeced$' /proc/cmdline) ]; then
  kexec --load /boot/vmlinuz-custom --initrd=/boot/initramfs.img \
    --reuse-cmdline --append=' kexeced'
  kexec --exec
fi

# If we made it this far we're running on the new kernel, trigger the original
# init binary with all the options passed too this as well as having it take
# over this process's PID.
exec /sbin/init.original &quot;$@&quot;
</pre></div>
</div>

<p>After rebooting you should be in your new kernel which you can verify with
<code>uname -a</code> and also by examining the <code>/proc/cmdline</code> file for the existence of
the &#39;kexeced&#39; flag.</p>

<p>If you modify the script above, be very careful as any execution error will
cause your system to die and recovery will only be possible by mounting the
filesystem on another linux system and fixing it.</p>

<p>In a future article I&#39;ll cover how to use this trick to build an active /
backup system allowing you to fall back to a known good system when booting
fails which is incredibly useful for embedded devices in the field that need
updates but are not easy to get too or replace when an update bricks the
system.</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2014/04/disabling-gnomes-keyring-in-fedora-19/">Disabling Gnome's Keyring in Fedora 19</a></h1>
      <aside>Posted at: 2014-04-14 10:19:23 -0400</aside>
      <article>
        <p>An update too Fedora a while ago started causing some unexpected behavior with
my dotfiles. Specifically the way I was handling my SSH agent. My SSH keys when
added to my agent automatically expire after a couple of hours.</p>

<p>After the update, when that expiration came I started receiving errors in my
shell that looked similar to the following (Since I fixed it I am not able to
get the exact working again):</p>
<div class="CodeRay">
  <div class="code"><pre>Warning: Unable to connect to SSH agent
</pre></div>
</div>

<p>I also noticed that periodically I got a Gnome keyring popup asking for my SSH
agent rather than my command-line client. I&#39;m personally not a big fan of
Gnome, but I deal with because it&#39;s the default for Fedora, tends to stay out
of your way, and switching to something else is just not a project I&#39;ve had
time for.</p>

<p>Now Gnome was very much getting in my way. I dealt with it for several months
now and finally got sick of it.</p>

<p>I tracked this down too the <code>gnome-keyring-daemon</code> which was starting up and
clobbering the contents of my <code>SSH_AUTH_SOCK</code> variable along with my
<code>GPG_AGENT_INFO</code> environment. Not very friendly.</p>

<p>There were a couple paths that I could&#39;ve gone for for solving this situation.
The first, and easiest way to probably have dealt with this was too put some
logic into my <code>~/.bashrc</code> file that detected when the <code>gnome-keying-agent</code> was
running, kill it and clean up after it. It might look something like this:</p>
<div class="CodeRay">
  <div class="code"><pre>if [ -n &quot;${GNOME_KEYRING_PID}&quot; ]; then
  if $(kill -0 ${GNOME_KEYRING_PID}); then
    kill ${GNOME_KEYRING_PID}
  fi
fi

unset GNOME_KEYRING_CONTROL SSH_AUTH_SOCK GPG_AGENT_INFO GNOME_KEYRING_PID
</pre></div>
</div>

<p>I share my dotfiles along a lot of different systems and don&#39;t like
system-specific behaivior getting in there. Instead I choose to find what was
starting up the keyring daemon and preventing it from doing so. Without a good
place to start and stubbornly refusing to Google this particular problem I took
the brute force approach of grep for the binary name in the <code>/etc</code> directory.</p>

<p>Sure enough in <code>/etc/xdg/autostart</code> I found a series of background daemons that
I definitely did not want nor need running. As root I ran the following command
to purge them from my system:</p>
<div class="CodeRay">
  <div class="code"><pre>cd /etc/xdg/autostart
rm -f gnome-keyring-{gpg,pkcs11,secrets,ssh}.desktop \
  gnome-welcome-tour.desktop imsettings-start.desktop \
  evolution-alarm-notify.desktop caribou-autostart.desktop
</pre></div>
</div>

<p>Make sure you read through that command as I&#39;m deleting more services than just
the Gnome keyring related daemons. The first solution will keep your system in
a default state, but this will permanently prevent the obnoxious behaivior on
your system for all users and prevents you from adding hacks to your bashrc to
work around mis-behaving software.</p>

<p>I hope this helps someone else!</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2014/08/dependency-prelink-issues/">Dependency Prelink Issues</a></h1>
      <aside>Posted at: 2014-08-12 16:16:14 -0400</aside>
      <article>
        <p>While running an aide check on one of my servers after updating it, I started
seeing a large number of very concerning warning messages:</p>
<div class="CodeRay">
  <div class="code"><pre>/usr/sbin/prelink: /bin/mailx: at least one of file's dependencies has changed since prelinking
Error on exit of prelink child process
/usr/sbin/prelink: /bin/rpm: at least one of file's dependencies has changed since prelinking
Error on exit of prelink child process
/usr/sbin/prelink: /sbin/readahead: at least one of file's dependencies has changed since prelinking
Error on exit of prelink child process
/usr/sbin/prelink: /lib64/libkrb5.so.3.3: at least one of file's dependencies has changed since prelinking
Error on exit of prelink child process
/usr/sbin/prelink: /lib64/libgssapi_krb5.so.2.2: at least one of file's dependencies has changed since prelinking
</pre></div>
</div>

<p>The list went on with maybe a total of forty packages and libraries. My initial
reaction was &#39;Did I get hacked?&#39;. Before running the updates I ran an aide
verification check which returned no issues and the files that were now
displaying the issue were in the packages that got updated.</p>

<p>What was the next worse scenario? The packages had been tampered with and I
just installed malicious files. This didn&#39;t seem likely as the packages are all
signed with GPG and an aide check would have caught tampering with my trust
database, the gpg binary, or the aide binary. Still a key could have been
comprimised.</p>

<p>After some Googling I came across people with similar issues, (including one
annoyingly paywalled RedHat article on the issue). Several people simply ended
the conversation on the assumption the user with the issue had been hacked.
Finally I <a href="http://lists.centos.org/pipermail/centos/2007-December/049222.html">came across one helpful individual</a> with the fix. The binaries
just need to have their prelink cache updated again. This can be accomplished
with the following command on CentOS 6.5 (probably the same on others).</p>
<div class="CodeRay">
  <div class="code"><pre>/usr/sbin/prelink -av -mR
</pre></div>
</div>

<p><em>Update:</em> Ultimately I decided to follow <a href="/knowledge_base/linux/hardening/">my own advice</a> (search for
prelink) and just simply disabled prelinking too prevent it from interferring
with aide checks and causing other weird issues. The memory trade-off isn&#39;t
valuable enough for me.</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2009/12/open-source-firewall-reviews-intro-and-pfsense/">Open Source Firewall Reviews: Intro & pfSense</a></h1>
      <aside>Posted at: 2009-12-16 10:58:15 -0500</aside>
      <article>
        <p>Every now and then I like to refresh parts of my home network. New technology
comes out, new versions of software come out and new exploits and attacks come
out. This time around I felt it was about time that I look at the various
firewall distributions that have come out in the past couple of years. I&#39;m
going to perform various reviews over the course of the next few weeks time
(and memory) permitting.</p>

<p>I have a few slightly more obscure requirements for my firewall, some of which
very few reviewers touch on. These being support for 802.1q VLAN tagging, a
built in traffic shaper, a tftp server and options to broadcast it as a PXE
boot server in the DHCP options, a captive portal on an arbitrary interface,
and last but not least updates that don&#39;t require a full system rebuild.</p>

<p>The more common features on my requirements are port forwarding, stateful
firewall with per interface rules, remote syslog logging, DHCP server, DMZ, and
VPN access (perferably PPTP and OpenVPN).</p>

<p>For a long time I&#39;ve been an avid supporter of <a href="http://pfsense.org/">pfSense</a>. For those of you
not interested in following the link, pfSense is a BSD firewall/router based on
a very strong distribution called m0n0wall. pfSense is in a nutshell a
frankenstein of some of the best pieces of the open source BSDs out there all
wrapped in a bunch of PHP based scripts.</p>

<p>As much as I love pfSense there have been a few issues that I&#39;ve grown to live
with. The first being a large number of NAT issues. I&#39;m not really sure how to
classify this but it is very apparent if you play any multi-player online games
(and exponentially more apparent if you have more than one person tying to play
online at the same time). I&#39;m going to use Company of Heroes and Call of Duty:
Modern Warfare 2 as examples for this.</p>

<p>Right off the bat connecting online in Modern Warfare 2, it will display your
&quot;NAT Status&quot; as either open or closed. When one person is connected it is
usually &quot;Open&quot;, when two people are connected one or both will display
&quot;Closed&quot;. Now this isn&#39;t a terribly big issue for MW2 as even with a closed NAT
you can still play the game perfectly fine. The trick is that you won&#39;t be able
to join any games (or even get into a lobby) with someone that is on the same
network as you. So much for working together.</p>

<p>With Company of Heroes things get considerably worse. Even with only one person
trying to play the game, whenever hosting or joining an online game about 50%
of the individual players you try and connect to or that try to connect to you
will fail with a &quot;NAT negotiation error&quot;. This makes playing or hosting
anything more than a 1v1 a royal pain, and more than a 2v2 nigh impossible.</p>

<p>Using a commercial off-the-shell router (in this case an old D-Link 524 I had
laying around) these issues go away completely. At the time of this writing I
am using version 1.2.3-RELEASE (built on Sun Dec 6 23:38:21 EST 2009). The NAT
issues are known and you can find many people with the same issues complaining
in their forums. The answer is almost always use static NAT mapping, but this
only works for one player. It does seem to solve both issues though (as long as
only one person being able to play is acceptable).</p>

<p>The next issue I&#39;ve run across seems to have something to do with the state
table. Now this is a tricky issue because it might be some dirty trick that
Comcast is playing on me and I haven&#39;t tested any other firewall distribution
yet to see where the problem lies.</p>

<p>Right off the bat, pfSense has a maximum state table size of 10,000. This is
WAY more than enough for any home or small business networks. With five people
behind it, I&#39;ve only seen the state table jump as high as 1,500 and that was
with all of us running torrents. The problem seems to be that anymore than 900
entries in the state table cause severe degredation in performance. How severe?
With a state table of 958, it took 43 seconds for text to be echoed back to me
over an SSH connection. That&#39;s impossibly bad latency. This issue is quickly
resolved by blowing away the state table or rebooting the firewall, but will
quickly crop back up when the computers start re-establishing the connections.
The built in traffic shaper only seems to make this problem worse not better.
(As an aside I&#39;m running pfSense on 1.6Ghz box with 512 Mb of ram, this is far
more than the minimum specs of 100Mhz and 128Mb of ram.)</p>

<p>Also while pfSense officially supports having a captive portal on a interface,
I&#39;ve only been able to get this working for a short period of time and that was
back in version 1.21. So beware anyone wanting to use pfSense for a hotspot.
The only reason I really wanted a captive portal was so that I could broadcast
a second wireless AP that the public could connect to anytime and they would be
able to see a kind notice asking them not to abuse the bandwidth I&#39;m freely
sharing.</p>

<p>So with these issues why have I stayed with pfSense for so long? It is a
fantastic, stable system when you don&#39;t need to worry about torrents or gaming.
There are quite a few packages that can be one click installed through the web
interface. It provides good stats about the status of the system in a clean and
easy to navigate interface.</p>

<p>pfSense has been doing a wonderful job of meeting all of my requirements with
the exception of the captive portal but that wasn&#39;t <u>really</u> for me anyway, the
gaming issues, and of course the state table. For anybody out there looking for
a solid well rounded firewall don&#39;t let my issues deter you. There is a version
2 in the works and is currently available that might solve all of my issues,
although it&#39;s alpha software right now and I didn&#39;t really want to have to
troubleshoot my home network all the time due to buggy software when I&#39;m trying
to relax.</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2013/12/using-dnsmasq-as-a-standalone-tftp-server/">Using Dnsmasq as a Standalone TFTP Server</a></h1>
      <aside>Posted at: 2013-12-12 18:29:46 -0500</aside>
      <article>
        <p><em>If you&#39;ve come across this blog post with the intention of setting up TFTP on
an modern version of OpenWRT I have a <a href="/blog/2014/07/using-openwrts-dnsmasq-as-a-tftp-server/">more recent blog post</a> detailing how
too configure your system.</em></p>

<p>I found myself in need of a TFTP server but wanted to avoid having all of the
xinet.d packages and services on my system (even if they were disabled). While
looking for alternatives I found out that <code>dnsmasq</code> has a built-in read-only
TFTP server.</p>

<p>I already have a DNS and DHCP server on my network and didn&#39;t want dnsmasq to
take on either of those roles so my first challenge was finding a way to
prevent dnsmasq from running those bits of it&#39;s code, or failing that I would
just firewall off the service. Luckily it&#39;s quite easy to disable both bits of
funtionality.</p>

<p>For DHCP you simply have to leave out any of the dhcp option in the
configuation file, DNS you just tell it to operate on port 0 and it will be
disabled.</p>

<p>So my whole config starting out looks like this:</p>
<div class="CodeRay">
  <div class="code"><pre># Disable DNS
port=0
</pre></div>
</div>

<p>Now I need to configure the TFTP bits of dnsmasq. This too was rather simple
only requiring me to add the following to my already terse config file:</p>
<div class="CodeRay">
  <div class="code"><pre># Enable the TFTP server
enable-tftp
tftp-root=/var/lib/tftp
</pre></div>
</div>

<p>I created the root directory for my TFTP server and started it up with the
following commands:</p>
<div class="CodeRay">
  <div class="code"><pre>mkdir /var/lib/tftp
systemctl enable dnsmasq.service
systemctl start dnsmasq.service
</pre></div>
</div>

<p>Voila, TFTP running and happy. If you have a firewall running you&#39;ll also want
to open ports 69/tcp and 69/udp (though I suspect only the UDP one is needed).</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2014/03/one-liner-ssl-certificate-generation/">One-Liner SSL Certificate Generation</a></h1>
      <aside>Posted at: 2014-03-28 14:52:51 -0400</aside>
      <article>
        <p>I regularily find myself in need of generating a quick SSL key and certificate
pair. I&#39;ve been using a one-liner for a while to generate these certificates.
No annoying user prompts just a quick fast certificate pair.</p>
<div class="CodeRay">
  <div class="code"><pre>echo -e &quot;XX\n\n \n \n\n*\n\n&quot; | openssl req -new -x509 -newkey rsa:2048 \
  -keyout service.key -nodes -days 90 -out service.crt &amp;&gt; /dev/null
</pre></div>
</div>

<p>A few notes about this, it is an ultimate wildcard matching any and all
hostnames with no location specific information, it should under no
circumstances be used for a production service. It&#39;s a 2048 bit key and only
valid for for roughly three months.</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2014/08/fast-hex-to-decimal-in-bash/">Fast Hex to Decimal in Bash</a></h1>
      <aside>Posted at: 2014-08-01 19:50:24 -0400</aside>
      <article>
        <p>I needed too turn some hexidecimal values into decimal in a bash script and
found a real easy way too do it. The following is a very short bash script
demostrating how too turn the hexidecimal string &quot;deadbeefcafe&quot; into it&#39;s
equivalent decimal value of &quot;244837814094590&quot;.</p>
<div class="CodeRay">
  <div class="code"><pre>#!/bin/bash

INPUT=&quot;deadbeefcafe&quot;
OUTPUT=$((0x${INPUT}))

echo $OUTPUT
</pre></div>
</div>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2014/02/creating-crypt-style-sha512-passwords-with-ruby/">Creating Crypt Style SHA512 Passwords With Ruby</a></h1>
      <aside>Posted at: 2014-02-17 15:28:27 -0500</aside>
      <article>
        <p>I needed to generate crypt-style SHA512 passwords in ruby for an <code>/etc/shadow</code>
file. After a bunch of Googling and messing around with the OpenSSL library I
finally found a very simple built-in way to handle this.</p>
<div class="CodeRay">
  <div class="code"><pre>require <span class="string"><span class="delimiter">'</span><span class="content">securerandom</span><span class="delimiter">'</span></span>

<span class="string"><span class="delimiter">'</span><span class="content">password</span><span class="delimiter">'</span></span>.crypt(<span class="string"><span class="delimiter">'</span><span class="content">$6$</span><span class="delimiter">'</span></span> + <span class="constant">SecureRandom</span>.random_number(<span class="integer">36</span> ** <span class="integer">8</span>).to_s(<span class="integer">36</span>))
</pre></div>
</div>

<p>You&#39;ll get a string that looks like:</p>
<div class="CodeRay">
  <div class="code"><pre>$6$4dksjo1b$Lt194Dwy7r/7WbM8MezYZysmGcxjaiisgTrTBbHkyBZFXeqQTG0J5hep4wLM/AmYxlGNLRy0OWATLDZCqjwCk.
</pre></div>
</div>

<p>If you don&#39;t want to use the <code>SecureRandom</code> module you can replace the random
call with simply <code>rand(36 ** 8)</code> though this isn&#39;t recommended.</p>

<p>Enjoy!</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2015/02/unbuffered-pipe-filters/">Unbuffered Pipe Filters</a></h1>
      <aside>Posted at: 2015-02-23 12:49:13 -0500</aside>
      <article>
        <p>I need to filter a live logstream for only relevant events and quickly hit an
issue that I wasn&#39;t expecting. The <code>grep</code> in my pipe chain was waiting until it
received all the output from the prior command before it began to attempt to
filter it.</p>

<p>Reading through the grep man page I came across the <code>--line-buffered</code> flag
which provides exactly what I needed. I wasn&#39;t using the <code>tail</code> command but it
serves really well in this situation to demonstrate the use:</p>
<div class="CodeRay">
  <div class="code"><pre>tail -f /var/log/maillog | grep --line-buffered -i error
</pre></div>
</div>

<p>Hope this saves someone a headache in the future!</p>

      </article>
    </div>
  
    <div class='post'>
      <h2><a href="/blog/2013/11/backing-up-gmail-with-fetchmail/">Backing up Gmail with fetchmail</a></h1>
      <aside>Posted at: 2013-11-19 09:55:40 -0500</aside>
      <article>
        <p>This morning I found myself in need of a large set of emails to test a
particular set of code. Ideally these emails would be broken out into easily
digestable pieces, and it was strictly for my own personal testing so I wasn&#39;t
concerned with using my own live data for this test (There will probably be
another post on this project later on).</p>

<p>Having used <code>fetchmail</code> with good results in the past I decided it was a good
idea to take this opportunity to also backup my Gmail account into the common
Maildir format (which essentially breaks out emails into individual files
meeting my requirements).</p>

<p>The first step was to enable POP access to my account through Gmail&#39;s
interface. You can accomplish this with the following steps.</p>

<ol>
<li>Login to Gmail</li>
<li>Click on the gear icon</li>
<li>Choose settings</li>
<li>Forwarding and POP/IMAP</li>
<li>Enable POP for all mail</li>
<li>When messages are accessed with POP... Keep&quot;</li>
<li>Save Changes.</li>
</ol>

<p>Ensure you have <code>fetchmail</code> and <code>procmail</code> installed. For me on Fedora this can
be accomplished using yum by running the following commands:</p>
<div class="CodeRay">
  <div class="code"><pre>$ sudo yum install fetchmail procmail -y
</pre></div>
</div>

<p>We need to configure fetchmail to let it know where to retrieve our mail from.
This configuration file lives at <code>$HOME/.fetchmailrc</code>. By default fetchmail
will send all retrieved mail to the local SMTP server over a normal TCP
connection. This isn&#39;t necessary or ideal, rather we&#39;ll additionally supply a
local mail delivery agent (procmail) to handle processing the mail into the
Maildir format.</p>
<div class="CodeRay">
  <div class="code"><pre>poll pop.gmail.com
protocol pop3
timeout 300
port 995
username &quot;full_email@withdomain.tld&quot; password &quot;yourpassword&quot;
keep
ssl
sslcertck
sslproto TLS1
mda &quot;/usr/bin/procmail -m '/home/&lt;username&gt;/.procmailrc'&quot;
</pre></div>
</div>

<p>Be sure to set the permissions on the <code>.fetchmailrc</code> file to 0600:</p>
<div class="CodeRay">
  <div class="code"><pre>$ chmod 0600 $HOME/.fetchmailrc
</pre></div>
</div>

<p>We&#39;ll now need to configure procmail to properly deliver our mail to the local
<code>Maildir</code> folder. Procmail&#39;s configuration by default lives in
<code>$HOME/.procmailrc</code></p>
<div class="CodeRay">
  <div class="code"><pre>LOGFILE=$HOME/.procmail.log
MAILDIR=$HOME
VERBOSE=on

:0
Maildir/
</pre></div>
</div>

<p>With that done, simply run the <code>fetchmail</code> command. In my experience this can
take a while process and it seems like Google limits the number of emails you
can download at a time, so you may need to run the command a couple of times to
get all your emails.</p>

      </article>
    </div>
  
</div>

    </div>
  </body>
</html>
